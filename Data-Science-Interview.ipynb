{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Foundations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the two parameters that specify the Normal Distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume that $y_i \\sim N(\\mu_i, \\sigma_i)$ where $\\mu_i, \\sigma_i$ are unknown and you can only access quantiles of $y_i$, how would you estimate $\\mu_i, \\sigma_i$?** \n",
    "\n",
    "*HINT: What would be the minimum number of quantiles required to estimate $\\mu$, $\\sigma$ to learn a distribution over $y_i$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given feature vectors and your choice of any function approximator, how would you learn a distribution over your predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume you have $N$ observations of $x_i$ and you have a prior that $x_i \\sim N(\\mu_p, \\sigma_p)$, how would you leverage your observations and prior to learn the proper estimates for actual parameters for $x_i$'s distribution?. Let's say an analyst proposed the following probabilistic program that imposes an even more hierarchical prior on your parameters, like below, how would you evaluate the probabilistic program?**\n",
    "```\n",
    "model {\n",
    "  // prior distributions\n",
    "  mu ~ normal(mu_prior_loc, mu_prior_scale);\n",
    "  sigma ~ normal(sigma_prior_loc, sigma_prior_scale);\n",
    "\n",
    "  // likelihood\n",
    "  errors ~ normal(mu, sigma);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Foundations & System Design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you diagnose and fix overfitting in traditional supervised learning systems?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you diagnose and fix overfitting in settings of extreme non-stationarity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have a massively unbalanced dataset. How do you evaluate the result of your classifier in this setting? If the model is not learning the imbalanced dataset, what are somethings you can do to help it learn the minority labels?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your process and framework for fixing bugs in machine learning systems?**\n",
    "\n",
    "**Can you provide some examples of the hariest ML bug you had to fix to get your probablistic system to give proper outputs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have access to raw transaction level datasets of some of the largest organizations in the world. How would you use that transaction level information to build _interpretable_ forecasts for organizations' cash inflows/outflows?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging: Parallelizing Data Frame Operations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when using Pandas to perform data pre-processing, you encounter transformations that can not be vectorized. In many such cases, these operations are trivially parallelizable across either the row or column axis. In this exercise, we present one such case out of many that we have been encountering internally. Your job is to parallelize this transformation. If you are not able to achieve such speedups, you need to explain why Dask or the method of your choice canâ€™t provide the speed up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `dataframe.csv`, for each `id` and `date`, e.g. each row, calculate the `median` of all of the `values` associated with that `id` before the given `date` in that row. In unoptimized Pandas Pseudo code, this looks something like:  \n",
    "\n",
    "```\n",
    "\n",
    "    def get_median_value_for_id_before_given_date(ID: id, date: pd.to_datetime, cache: pd.DataFrame) -> float:\n",
    "        group = cache[(cache.id == ID) & (cache.date < date)]\n",
    "        return group['value'].median()\n",
    "        \n",
    "    dataframe['value_summary_stats'] = sample_df.apply(\n",
    "        lambda x: get_median_value_for_id_before_given_date(x['id'], x['date'], dataframe),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    " ```\n",
    " \n",
    " How would you speed up this unoptimized code? Feel free to write code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
